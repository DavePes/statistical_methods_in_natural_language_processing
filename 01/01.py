# -*- coding: utf-8 -*-
"""npfl147_du01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1elCHfyVRmsTPYD_69_erZl58sQh2l8Jw

Cell 01: Installations & Imports
"""

#!pip install datasets sacremoses tabulate transformers

from datasets import load_dataset
from sacremoses import MosesTokenizer
from collections import Counter
from math import log2
from tabulate import tabulate
from transformers import AutoTokenizer
import matplotlib.pyplot as plt
import numpy as np

"""Cell 2: Load Dataset"""

langs = ['cs', 'en', 'de']
datasets = {}

print("Loading datasets...")
for lang in langs:
    # Loading the dataset from Hugging Face
    dataset = load_dataset("ufal/npfl147", lang)
    datasets[lang] = dataset
    print(f"Loaded {lang}")

# Extracting text data
texts = [[x['text'] for x in data['train']] for data in datasets.values()]

"""Cell 3: Moses Tokenization"""

print("Tokenizing with Moses...")
def moses_tokenizer(texts):
  tokens = {}
  for i, lang in enumerate(langs):
      tok = MosesTokenizer(lang=lang)
      all_tokens = []
      # Iterate through sentences to tokenize them individually
      for sentence in texts[i]:
          all_tokens.extend(tok.tokenize(sentence))
      tokens[lang] = all_tokens
      print(f"Tokenized {lang}: {len(all_tokens)} tokens")
  return tokens
tokens = moses_tokenizer(texts)

"""Cell 4: Define Counting Function"""

def dataset_counter(token_dict):
    # Initialize dictionaries to hold stats for all languages
    data_sizes = {}
    unigrams_frequencies, unigrams_counts = {}, {}
    bigrams_frequencies, bigrams_counts = {}, {}
    trigrams_frequencies, trigrams_counts = {}, {}

    for lang in langs:
        current_tokens = token_dict[lang]
        bigram = Counter()
        trigram = Counter()

        # Sliding window loop: Iterate through tokens to find N-grams
        for i in range(len(current_tokens)-1):
            # Create bigram tuple (w1, w2) and count it
            bigram[(current_tokens[i], current_tokens[i+1])] += 1

            # Check if we have enough tokens left for a trigram (w1, w2, w3)
            if i < len(current_tokens)-2:
                trigram[(current_tokens[i], current_tokens[i+1], current_tokens[i+2])] += 1

        # Count frequencies of individual words (Unigrams)
        unigrams_frequencies[lang] = Counter(current_tokens)
        unigrams_counts[lang] = len(unigrams_frequencies[lang]) # Count of unique words

        # Store Bigram stats
        bigrams_frequencies[lang] = bigram
        bigrams_counts[lang] = len(bigrams_frequencies[lang]) # Count of unique bigrams

        # Store Trigram stats
        trigrams_frequencies[lang] = trigram
        trigrams_counts[lang] = len(trigrams_frequencies[lang]) # Count of unique trigrams

        # Total number of tokens in the dataset
        data_sizes[lang] = len(current_tokens)

    return (unigrams_frequencies, unigrams_counts,
            bigrams_frequencies, bigrams_counts,
            trigrams_frequencies, trigrams_counts,
            data_sizes)

# Run counter on Moses tokens
(unigrams_frequencies, unigrams_counts,
 bigrams_frequencies, bigrams_counts,
 trigrams_frequencies, trigrams_counts,
 data_sizes) = dataset_counter(tokens)

# Print checks to verify data was populated
print("Unigrams counts:", unigrams_counts)
print("Bigrams counts:", bigrams_counts)
print("Trigrams counts:", trigrams_counts)
print("Data sizes:", data_sizes)

"""Cell 5: Conditional Entropy"""

def bigram_entropy(bigrams_frequencies, unigrams_frequencies,unigrams_counts,bigrams_counts,trigrams_counts, data_sizes, label="Bigram Entropy"):
    entropies = {}
    table_data = []

    for lang in langs:
        entropies[lang] = 0
        # Iterate through every unique bigram found in the text
        for w1, w2 in bigrams_frequencies[lang]:

            # Joint probability P(w1, w2): How often this pair appears out of all pairs
            p_w1_w2 = bigrams_frequencies[lang][(w1, w2)] / data_sizes[lang]

            # Conditional probability P(w2 | w1): Given w1, how likely is w2?
            # Formula: Count(w1, w2) / Count(w1)
            p_w2_given_w1 = bigrams_frequencies[lang][(w1, w2)] / unigrams_frequencies[lang][w1]

            # Summing up entropy: - Σ P(joint) * log2(P(conditional))
            entropies[lang] -= p_w1_w2 * log2(p_w2_given_w1)

        row = [
            lang.upper(),
            unigrams_counts[lang],
            bigrams_counts[lang],
            trigrams_counts[lang],
            data_sizes[lang],
            f"{entropies[lang]:.4f}" # Format to 4 decimal places
        ]
        table_data.append(row)
    return table_data

# Calculate and print table
table_data = bigram_entropy(bigrams_frequencies, unigrams_frequencies,unigrams_counts,bigrams_counts,trigrams_counts, data_sizes)
print(tabulate(table_data, headers=["Language", "Unigrams", "Bigrams","Trigrams", "Data Size", "Entropy"]))

"""*How does the entropy compare among the languages? <br>
Which language has the highest entropy? which one lowest? <br>
Explain the difference*<br>
English has the highest entropy (5.6723) → it is the least predictable among the three. <br>
Czech has the lowest entropy (5.064) → it is the most predictable. <br>
German is in the middle (5.593). <br>
English has highest unigrams/bigrams ratio suggesting more combinations and variability → higher uncertainty → higher entropy

Cell 6: XLM-R Tokenization & Entropy
"""

print("Tokenizing with XLM-R...")
xlm_tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
xlm_tokens = {}

for i, lang in enumerate(langs):
    all_tokens = []
    for t in texts[i]:
        all_tokens.extend(xlm_tokenizer.tokenize(t))
    xlm_tokens[lang] = all_tokens

# Count XLM tokens
(xlm_uni_freq, xlm_uni_count,
 xlm_bi_freq, xlm_bi_count,
 xlm_tri_freq, xlm_tri_count,
 xlm_data_sizes) = dataset_counter(xlm_tokens)

# Calculate Entropy
table_data_xlm = bigram_entropy(xlm_bi_freq, xlm_uni_freq,xlm_uni_count,xlm_bi_count,xlm_tri_count,xlm_data_sizes, label="Bigram Entropy (XLM-R)")
print(tabulate(table_data_xlm, headers=["Language", "Unigrams", "Bigrams","Trigrams", "Data Size",  "Entropy (XLM-R)"]))

"""tabulate everything again -- how does entropy compare to the
baseline tokenizer? <br>

Czech (CS) : Baseline: 5.064 → XLM-R: 5.5088 <br>
English (EN): Baseline: 5.6723 → XLM-R: 5.3282 <br>
German (DE): Baseline: 5.593 → XLM-R: 5.6601 <br>

XLM-R tokenization works on subwords, not full words. <br>
This changes the distribution of tokens: <br>
languages with more compounds or rich morphology (like CS, DE) tend to show higher entropy, while languages with simpler word structures (like EN) can show lower entropy after subword tokenization.

Cell 7: Dataset Splitting
"""

texts_train = []
texts_val = []
texts_test = []
for i,_ in enumerate(langs):
  texts_train.append(texts[i][:700])
  texts_val.append(texts[i][700:900])
  texts_test.append(texts[i][900:])


tokens_train = moses_tokenizer(texts_train)
tokens_val = moses_tokenizer(texts_val)
tokens_test = moses_tokenizer(texts_test)

"""Cell 8: Statistics on Splits"""

# Statistics for Train
(train_uni_freq, train_uni_count,
 train_bi_freq, train_bi_count,
 train_tri_freq, train_tri_count,
 train_data_sizes) = dataset_counter(tokens_train)
# Bigram Entropy for Train
table_data_train = bigram_entropy(train_bi_freq, train_uni_freq,train_uni_count,train_bi_count,train_tri_count,train_data_sizes, label="Bigram Entropy (Train)")
print(tabulate(table_data_train, headers=["Language", "Unigrams", "Bigrams","Trigrams", "Data Size", "Bigram Entropy (Train)"]))

# Test Set
(test_uni_freq, test_uni_count,
 test_bi_freq, test_bi_count,
 test_tri_freq, test_tri_count,
 test_data_sizes) = dataset_counter(tokens_test)

# Bigram Entropy for Test
table_data_test = bigram_entropy(test_bi_freq, test_uni_freq,test_uni_count,test_bi_count,test_tri_count,test_data_sizes, label="Bigram Entropy (Test)")
print(tabulate(table_data_test, headers=["Language", "Unigrams", "Bigrams","Trigrams", "Data Size",  "Bigram Entropy (Test)"]))

# Validation Set
(val_uni_freq, val_uni_count,
 val_bi_freq, val_bi_count,
 val_tri_freq, val_tri_count,
 val_data_sizes) = dataset_counter(tokens_val)

# Bigram Entropy for Validation
table_data_val = bigram_entropy(val_bi_freq, val_uni_freq,val_uni_count,val_bi_count,val_tri_count,val_data_sizes, label="Bigram Entropy (Validation)")
print(tabulate(table_data_val, headers=["Language", "Unigrams", "Bigrams","Trigrams", "Data Size",  "Bigram Entropy (Validation)"]))

# bundling for easier passing
test_stats = (test_uni_freq, test_uni_count, test_bi_freq, test_bi_count, test_tri_freq, test_tri_count, test_data_sizes)
train_stats = (train_uni_freq, train_uni_count, train_bi_freq, train_bi_count, train_tri_freq, train_tri_count, train_data_sizes)


for i,t in enumerate(tokens_test['cs']):
    if t == "průzkum" or t == "veřejného" or t == "mínění":
        print(f"Found '{t}' in TEST tokens for {langs[i]}")
# trigram prob
def calculate_raw_trigram_prob(stats, lang, w1, w2, w3):
    # Unpack the training statistics
    (uni_freq, uni_count, bi_freq, bi_count, tri_freq, tri_count, data_sizes) = stats

    # 1. Get Numerator: Count of the full trigram "w1 w2 w3"
    count_trigram = tri_freq[lang].get((w1, w2, w3), 0)

    # 2. Get Denominator: Count of the history "w1 w2"
    count_history = bi_freq[lang].get((w1, w2), 0)

    print(f"--- Analysis for ({lang.upper()}) '{w1} {w2} {w3}' ---")
    print(f"Count({w1}, {w2}, {w3}) = {count_trigram}")
    print(f"Count({w1}, {w2})      = {count_history}")

    # 3. Compute Probability (Handling OOV History)
    if count_history == 0:
        print("Probability: 0.0 (Reason: OOV History - The phrase '%s %s' was never seen in training)" % (w1, w2))
        return 0.0
    else:
        prob = count_trigram / count_history
        print(f"Probability: {prob:.6f}")
        return prob

# --- 1. English Test ---
# Phrase: "beginning of the"
calculate_raw_trigram_prob(test_stats, 'en', 'beginning', 'of', 'the')

print("\n")

# --- 2. Czech Test ---
# Phrase: "průzkum veřejného mínění"
calculate_raw_trigram_prob(test_stats, 'cs', 'průzkum', 'veřejného', 'mínění')

"""Cell 9: Probability & Interpolation Functions"""

def compute_probs(stats):
    # Unpack the statistics tuple
    uni_freq, uni_count, bi_freq, bi_count, tri_freq, tri_count, data_sizes = stats

    unigram_probs = {}
    bigram_probs = {}
    trigram_probs = {}

    for lang in langs:
        unigram_probs[lang] = {}
        bigram_probs[lang] = {}
        trigram_probs[lang] = {}

        # 1. Unigram Probabilities
        # P(w) = Count(w) / Total_Tokens
        for w1 in uni_freq[lang]:
            p_w1 = uni_freq[lang][w1] / data_sizes[lang]
            unigram_probs[lang][w1] = p_w1

        # 2. Bigram Probabilities
        # P(w2 | w1) = Count(w1, w2) / Count(w1)
        for w1, w2 in bi_freq[lang]:
            p_w2_given_w1 = bi_freq[lang][(w1, w2)] / uni_freq[lang][w1]
            bigram_probs[lang][(w1, w2)] = p_w2_given_w1

        # 3. Trigram Probabilities
        # P(w3 | w1, w2) = Count(w1, w2, w3) / Count(w1, w2)
        for w1, w2, w3 in tri_freq[lang]:
            p_w3_given_w1_w2 = tri_freq[lang][(w1, w2, w3)] / bi_freq[lang][(w1, w2)]
            trigram_probs[lang][(w1, w2, w3)] = p_w3_given_w1_w2

    return unigram_probs, bigram_probs, trigram_probs

def interpolated_cross_entropy(tokens_data, unigram_probs, bigram_probs, trigram_probs, lambdas):
    """
    Calculates cross-entropy on TEST data using probabilities from TRAIN data.
    """
    entropies = []
    for lang in langs:
        # Unpack lambda weights for this language
        l3, l2, l1, l0 = lambdas[lang]
        log_prob_sum = 0

        N = len(tokens_data[lang])
        # V = Vocabulary size (from training data) used for Uniform smoothing
        V = len(unigram_probs[lang])

        # Loop through the text (starting at index 2 to have enough context for trigrams)
        for i in range(2, N):
            w1 = tokens_data[lang][i-2] # Context
            w2 = tokens_data[lang][i-1] # Context
            w3 = tokens_data[lang][i]   # Target Word

            p_tri = trigram_probs[lang].get((w1, w2, w3), 0.0)
            p_bi  = bigram_probs[lang].get((w2, w3), 0.0)
            p_uni = unigram_probs[lang].get(w3, 0.0)
            p_zero = 1 / V # Uniform probability

            # Linear Interpolation formula
            p_combined = (l3 * p_tri) + (l2 * p_bi) + (l1 * p_uni) + (l0 * p_zero)

            # Sum negative log probabilities (Cross Entropy definition)
            if p_combined > 0:
                log_prob_sum -= log2(p_combined)
            else:
                # This theoretically shouldn't happen
                pass

        # Average the sum over the number of predictions made
        entropy = log_prob_sum / (N - 2)
        entropies.append((lang, entropy))

    return entropies

"""Cell 10: EM Algorithm"""

def em_optimal_lambdas(tokens_source, unigram_probs, bigram_probs, trigram_probs, epsilon=1e-3, default_lambdas=[0.25, 0.25, 0.25, 0.25]):
    """
    Optimizes interpolation weights (lambdas) using the EM algorithm.
    """
    lambdas_for_all_langs = {}

    for lang in langs:
        lambdas = default_lambdas.copy()

        # N = Total words in the held-out (validation) set
        N = len(tokens_source[lang])
        # V = Vocabulary size (derived from Training data) for Uniform smoothing
        V = len(unigram_probs[lang])

        diff = float('inf')

        # Loop until the change in lambdas is very small
        while diff > epsilon:
            expected_counts = [0.0, 0.0, 0.0, 0.0]

            # --- E-STEP (Expectation) ---
            for i in range(2, N):
                w3 = tokens_source[lang][i]     # Target
                w2 = tokens_source[lang][i-1]   # Context
                w1 = tokens_source[lang][i-2]   # Context

                # Get the raw probability from each model
                p_tri = trigram_probs[lang].get((w1, w2, w3), 0.0)
                p_bi  = bigram_probs[lang].get((w2, w3), 0.0)
                p_uni = unigram_probs[lang].get(w3, 0.0)
                p0 = 1.0 / V

                probs = [p_tri, p_bi, p_uni, p0]

                # Calculate the current weighted probability of the word
                p_combined = (lambdas[0] * p_tri) + (lambdas[1] * p_bi) + (lambdas[2] * p_uni) + (lambdas[3] * p0)

                if p_combined > 0:
                    # Calculate expected_counts
                    for j in range(4):
                        expected_counts[j] += (lambdas[j] * probs[j]) / p_combined

            # --- (Maximization) ---
            # Update weights to maximize likelihood of the observed data
            total_count = sum(expected_counts)

            if total_count == 0: break # Safety check

            lambdas_new = [count / total_count for count in expected_counts]

            # Calculate how much the lambdas changed
            diff = sum(abs(lambdas_new[j] - lambdas[j]) for j in range(4))

            # Update for next iteration
            lambdas = lambdas_new

        lambdas_for_all_langs[lang] = lambdas
        print(f"Optimized Lambdas for {lang}: {[round(l, 4) for l in lambdas]}")

    return lambdas_for_all_langs

"""Cell 11: Run Optimization and Compare"""

# Compute probabilities from training and validation data
train_uni_probs, train_bi_probs, train_tri_probs = compute_probs(train_stats)

# 1. Cross Entropy with Default Lambdas
default_l = {lang: [0.25, 0.25, 0.25, 0.25] for lang in langs}
entropy_default_lambdas = interpolated_cross_entropy(
    tokens_test,
    train_uni_probs, train_bi_probs, train_tri_probs,
    lambdas=default_l
)

# 2. Optimize Lambdas using EM with validation (heldout) dataset
best_lambdas = em_optimal_lambdas(tokens_val, train_uni_probs, train_bi_probs, train_tri_probs)


# 3. Cross Entropy with Best Lambdas with test
entropy_best_lambdas = interpolated_cross_entropy(
    tokens_test,
    train_uni_probs, train_bi_probs, train_tri_probs,
    lambdas=best_lambdas
)

print("\n--- Results ---")
print("Cross-Entropy with Default Lambdas (0.25 each):")
for lang, entropy in entropy_default_lambdas:
    print(f"Language: {lang.upper()}, Cross-Entropy: {entropy:.4f}")

print("\nCross-Entropy with Best Lambdas (EM Optimized):")
for lang, entropy in entropy_best_lambdas:
    print(f"Language: {lang.upper()}, Cross-Entropy: {entropy:.4f}")

"""Cell 12: Visualization"""

lambda3_values = np.linspace(0, 0.95, 20)
cross_entropies = {lang: [] for lang in langs}

train_uni_probs, train_bi_probs, train_tri_probs = compute_probs(train_stats)

for lang in langs:
    best_l3, best_l2, best_l1, best_l0 = best_lambdas[lang]

    for l3 in lambda3_values:
        remaining = 1 - l3

        # Re-normalize the other lambdas proportionally
        total_other = best_l2 + best_l1 + best_l0

        if total_other == 0:
            l2, l1, l0 = 0, 0, 0
        else:
            l2 = (best_l2 / total_other) * remaining
            l1 = (best_l1 / total_other) * remaining
            l0 = (best_l0 / total_other) * remaining

        # Construct temporary lambda dictionary just for this step
        temp_lambdas = {l: [l3, l2, l1, l0] if l == lang else best_lambdas[l] for l in langs}

        # Calculate entropy for test data
        entropy_results = interpolated_cross_entropy(
            tokens_test,
            train_uni_probs, train_bi_probs, train_tri_probs,
            lambdas=temp_lambdas
        )

        # Extract the entropy for the current language
        lang_entropy = next(e for l, e in entropy_results if l == lang)
        cross_entropies[lang].append(lang_entropy)

# Plotting
plt.figure(figsize=(10, 6))
for lang in langs:
    plt.plot(lambda3_values, cross_entropies[lang], label=lang.upper(), marker='.')

plt.xlabel('Lambda 3 (Trigram Weight)')
plt.ylabel('Cross-Entropy')
plt.title('Cross-Entropy vs Lambda 3 (Interpolated)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

"""how does cross-entropy change with lambda 3 and why? <br>

It appears that the best lambda 3 is 0, or very close to zero. This may be because trigrams represent very specific sequences, and using them effectively requires a large dataset.<br>
Are there systematic differences across languages? <br>
We do not observe any significant systematic differences across languages.
"""
